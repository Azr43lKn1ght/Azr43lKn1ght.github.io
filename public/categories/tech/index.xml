<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Tech on Azr43lkn1ght</title>
    <link>http://localhost:1313/categories/tech/</link>
    <description>Recent content in Tech on Azr43lkn1ght</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 15 Apr 2024 05:51:15 +0530</lastBuildDate>
    <atom:link href="http://localhost:1313/categories/tech/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>谈谈微服务架构中的基础设施：Service Mesh与Istio</title>
      <link>http://localhost:1313/2018/03/29/what-is-service-mesh-and-istio/</link>
      <pubDate>Thu, 29 Mar 2018 12:00:00 +0000</pubDate>
      <guid>http://localhost:1313/2018/03/29/what-is-service-mesh-and-istio/</guid>
      <description>微服务架构的演进作为一种架构模式，微服务将复杂系统切分为数十乃至上百个小服务，每个服务负责实现一个独立的业务逻辑。这些小服务易于被小型的软件工程师团队所理解和修改，并带来了语言和框架选择灵活性，缩短应用开发上线时间，可根据不同的工作负载和资源要求对服务进行独立缩扩容等优势。&#xA;另一方面，当应用被拆分为多个微服务进程后，进程内的方法调用变成了了进程间的远程调用。引入了对大量服务的连接、管理和监控的复杂性。&#xA;该变化带来了分布式系统的一系列问题，例如：&#xA;如何找到服务的提供方？ 如何保证远程方法调用的可靠性？ 如何保证服务调用的安全性？ 如何降低服务调用的延迟？ 如何进行端到端的调试？ 另外生产部署中的微服务实例也增加了运维的难度,例如：&#xA;如何收集大量微服务的性能指标已进行分析？ 如何在不影响上线业务的情况下对微服务进行升级？ 如何测试一个微服务集群部署的容错和稳定性？ 这些问题涉及到成百上千个服务的通信、管理、部署、版本、安全、故障转移、策略执行、遥测和监控等，要解决这些微服务架构引入的问题并非易事。&#xA;让我们来回顾一下微服务架构的发展过程。在出现服务网格之前，我们最开始在微服务应用程序内理服务之间的通讯逻辑，包括服务发现，熔断，重试，超时，加密，限流等逻辑。&#xA;在一个分布式系统中，这部分逻辑比较复杂，为了为微服务应用提供一个稳定、可靠的基础设施层，避免大家重复造轮子，并减少犯错的可能，一般会通过对这部分负责服务通讯的逻辑进行抽象和归纳，形成一个代码库供各个微服务应用程序使用，如下图所示：&#xA;公共的代码库减少了应用程序的开发和维护工作量，降低了由应用开发人员单独实现微服务通讯逻辑出现错误的机率，但还是存在下述问题：&#xA;微服务通讯逻辑对应用开发人员并不透明，应用开发人员需要理解并正确使用代码 库，不能将其全部精力聚焦于业务逻辑。 需要针对不同的语言/框架开发不同的代码库，反过来会影响微服务应用开发语言 和框架的选择，影响技术选择的灵活性。 随着时间的变化，代码库会存在不同的版本，不同版本代码库的兼容性和大量运行 环境中微服务的升级将成为一个难题。 可以将微服务之间的通讯基础设施层和 TCP/IP 协议栈进行类比。TCP/IP 协议栈为操作系统中的所有应用提供基础通信服务，但 TCP/IP 协议栈和应用程序之间并没有紧密的耦合关系，应用只需要使用 TCP/IP 协议提供的底层通讯功能,并不关心 TCP/IP 协议的实现，如 IP 如何进行路由，TCP 如何创建链接等。&#xA;同样地，微服务应用也不应该需要关注服务发现，Load balancing，Retries，Circuit Breaker 等微服务之间通信的底层细节。如果将为微服务提供通信服务的这部分逻辑从应用程序进程中抽取出来，作为一个单独的进程进行部署，并将其作为服务间的通信代理，可以得到如下图所示的架构：&#xA;因为通讯代理进程伴随应用进程一起部署，因此形象地把这种部署方式称为“sidecar”/边车（即三轮摩托的挎斗）。&#xA;应用间的所有流量都需要经过代理，由于代理以 sidecar 方式和应用部署在同一台主机上，应用和代理之间的通讯可以被认为是可靠的。由代理来负责找到目的服务并负责通讯的可靠性和安全等问题。&#xA;当服务大量部署时，随着服务部署的 sidecar 代理之间的连接形成了一个如下图所示的网格，该网格成为了微服务的通讯基础设施层，承载了微服务之间的所有流量，被称之为 Service Mesh（服务网格）。&#xA;_服务网格是一个基础设施层，用于处理服务间通信。云原生应用有着复杂的服务拓扑，服务网格保证请求可以在这些拓扑中可靠地穿梭。在实际应用当中，服务网格通常是由一系列轻量级的网络代理组成的，它们与应用程序部署在一起，但应用程序不需要知道它们的存在。&#xA;_William Morgan _&#xD;WHAT’S A SERVICE MESH? AND WHY DO I NEED ONE? _&#xA;服务网格中有数量众多的 Sidecar 代理，如果对每个代理分别进行设置，工作量将非常巨大。为了更方便地对服务网格中的代理进行统一集中控制，在服务网格上增加了控制面组件。&#xA;这里我们可以类比 SDN 的概念，控制面就类似于 SDN 网管中的控制器，负责路由策略的指定和路由规则下发；数据面类似于 SDN 网络中交换机，负责数据包的转发。</description>
    </item>
    <item>
      <title>如何从外部访问Kubernetes集群中的应用？</title>
      <link>http://localhost:1313/2017/11/28/access-application-from-outside/</link>
      <pubDate>Tue, 28 Nov 2017 12:00:00 +0000</pubDate>
      <guid>http://localhost:1313/2017/11/28/access-application-from-outside/</guid>
      <description>前言我们知道，kubernetes 的 Cluster Network 属于私有网络，只能在 cluster Network 内部才能访问部署的应用，那如何才能将 Kubernetes 集群中的应用暴露到外部网络，为外部用户提供服务呢？本文探讨了从外部网络访问 kubernetes cluster 中应用的几种实现方式。&#xA;本文尽量试着写得比较容易理解，但要做到“深入浅出”，把复杂的事情用通俗易懂的语言描述出来是非常需要功力的，个人自认尚未达到此境界，唯有不断努力。此外，kubernetes 本身是一个比较复杂的系统，无法在本文中详细解释涉及的所有相关概念，否则就可能脱离了文章的主题，因此假设阅读此文之前读者对 kubernetes 的基本概念如 docker，container，pod 已有所了解。&#xA;另外此文中的一些内容是自己的理解，由于个人的知识范围有限，可能有误，如果读者对文章中的内容有疑问或者勘误，欢迎大家指证。&#xA;Pod 和 Service我们首先来了解一下 Kubernetes 中的 Pod 和 Service 的概念。&#xA;Pod(容器组),英文中 Pod 是豆荚的意思，从名字的含义可以看出，Pod 是一组有依赖关系的容器，Pod 包含的容器都会运行在同一个 host 节点上，共享相同的 volumes 和 network namespace 空间。Kubernetes 以 Pod 为基本操作单元，可以同时启动多个相同的 pod 用于 failover 或者 load balance。&#xA;Pod 的生命周期是短暂的，Kubernetes 根据应用的配置，会对 Pod 进行创建，销毁，根据监控指标进行缩扩容。kubernetes 在创建 Pod 时可以选择集群中的任何一台空闲的 Host，因此其网络地址是不固定的。由于 Pod 的这一特点，一般不建议直接通过 Pod 的地址去访问应用。&#xA;为了解决访问 Pod 不方便直接访问的问题，Kubernetes 采用了 Service 的概念，Service 是对后端提供服务的一组 Pod 的抽象，Service 会绑定到一个固定的虚拟 IP 上，该虚拟 IP 只在 Kubernetes Cluster 中可见，但其实该 IP 并不对应一个虚拟或者物理设备，而只是 IPtable 中的规则，然后再通过 IPtable 将服务请求路由到后端的 Pod 中。通过这种方式，可以确保服务消费者可以稳定地访问 Pod 提供的服务，而不用关心 Pod 的创建、删除、迁移等变化以及如何用一组 Pod 来进行负载均衡。</description>
    </item>
    <item>
      <title>采用Istio实现灰度发布(金丝雀发布)</title>
      <link>http://localhost:1313/2017/11/08/istio-canary-release/</link>
      <pubDate>Wed, 08 Nov 2017 15:00:00 +0000</pubDate>
      <guid>http://localhost:1313/2017/11/08/istio-canary-release/</guid>
      <description>灰度发布（又名金丝雀发布）介绍当应用上线以后，运维面临的一大挑战是如何能够在不影响已上线业务的情况下进行升级。做过产品的同学都清楚，不管在发布前做过多么完备的自动化和人工测试，在发布后都会出现或多或少的故障。根据墨菲定律，可能会出错的版本发布一定会出错。&#xA;&amp;ldquo;ANYTHING THAN CAN GO WRONG WILL GO WRONG&amp;rdquo; &amp;ndash;MURPHY&amp;rsquo;S LAW&#xA;因此我们不能寄希望于在线下测试时发现所有潜在故障。在无法百分百避免版本升级故障的情况下，需要通过一种方式进行可控的版本发布，把故障影响控制在可以接受的范围内，并可以快速回退。&#xA;可以通过&#xD;灰度发布（又名金丝雀发布）&#xD;来实现业务从老版本到新版本的平滑过渡，并避免升级过程中出现的问题对用户造成的影响。&#xA;“金丝雀发布”的来源于矿工们用金丝雀对矿井进行空气测试的做法。以前矿工挖煤的时候，矿工下矿井前会先把金丝雀放进去，或者挖煤的时候一直带着金丝雀。金丝雀对甲烷和一氧化碳浓度比较敏感，会先报警。所以大家都用“金丝雀”来搞最先的测试。&#xA;下图中，左下方的少部分用户就被当作“金丝雀”来用于测试新上线的 1.1 版本。如果新版本出现问题，“金丝雀”们会报警，但不会影响其他用户业务的正常运行。&#xA;灰度发布（金丝雀发布）的流程如下：&#xA;准备和生产环境隔离的“金丝雀”服务器。 将新版本的服务部署到“金丝雀”服务器上。 对“金丝雀”服务器上的服务进行自动化和人工测试。 测试通过后，将“金丝雀”服务器连接到生产环境，将少量生产流量导入到“金丝雀”服务器中。 如果在线测试出现问题，则通过把生产流量从“金丝雀”服务器中重新路由到老版本的服务的方式进行回退，修复问题后重新进行发布。 如果在线测试顺利，则逐渐把生产流量按一定策略逐渐导入到新版本服务器中。 待新版本服务稳定运行后，删除老版本服务。 Istio 实现灰度发布(金丝雀发布)的原理从上面的流程可以看到，如果要实现一套灰度发布的流程，需要应用程序和运维流程对该发布过程进行支持，工作量和难度的挑战是非常大的。虽然面对的问题类似，但每个企业或组织一般采用不同的私有化实现方案来进行灰度发布,为解决该问题导致研发和运维花费了大量的成本。&#xA;Istio 通过高度的抽象和良好的设计采用一致的方式解决了该问题，采用 sidecar 对应用流量进行了转发，通过 Pilot 下发路由规则，可以在不修改应用程序的前提下实现应用的灰度发布。&#xA;备注：采用 kubernetes 的&#xD;滚动升级(rolling update)&#xD;功能也可以实现不中断业务的应用升级,但滚动升级是通过逐渐使用新版本的服务来替换老版本服务的方式对应用进行升级，在滚动升级不能对应用的流量分发进行控制，因此无法采用受控地把生产流量逐渐导流到新版本服务中，也就无法控制服务升级对用户造成的影响。&#xA;采用 Istio 后，可以通过定制路由规则将特定的流量（如指定特征的用户）导入新版本服务中，在生产环境下进行测试，同时通过渐进受控地导入生产流量，可以最小化升级中出现的故障对用户的影响。并且在同时存在新老版本服务时，还可根据应用压力对不同版本的服务进行独立的缩扩容，非常灵活。采用 Istio 进行灰度发布的流程如下图所示：&#xA;操作步骤下面采用 Istion 自带的 BookinfoInfo 示例程序来试验灰度发布的流程。&#xA;测试环境安装首先参考&#xD;手把手教你从零搭建 Istio 及 Bookinfo 示例程序&#xD;安装 Kubernetes 及 Istio 控制面。&#xA;因为本试验并不需要安装全部 3 个版本的 reviews 服务，因此如果已经安装了该应用，先采用下面的命令卸载。&#xA;istio-0.2.10/samples/bookinfo/kube/cleanup.sh 部署 V1 版本的服务首先只部署 V1 版本的 Bookinfo 应用程序。由于示例中的 yaml 文件中包含了 3 个版本的 reviews 服务，我们先将 V2 和 V3 版本的 Deployment 从 yaml 文件 istio-0.</description>
    </item>
  </channel>
</rss>
