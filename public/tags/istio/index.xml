<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Istio on Azr43lkn1ght</title>
    <link>http://localhost:1313/tags/istio/</link>
    <description>Recent content in Istio on Azr43lkn1ght</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 15 Apr 2024 05:51:15 +0530</lastBuildDate>
    <atom:link href="http://localhost:1313/tags/istio/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Istio v1aplha3 routing API介绍（译文）</title>
      <link>http://localhost:1313/2018/06/04/introducing-the-istio-v1alpha3-routing-api/</link>
      <pubDate>Mon, 04 Jun 2018 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/2018/06/04/introducing-the-istio-v1alpha3-routing-api/</guid>
      <description>&lt;p&gt;到目前为止，Istio 提供了一个简单的 API 来进行流量管理，该 API 包括了四种资源：RouteRule，DestinationPolicy，EgressRule 和 Ingress（直接使用了 Kubernets 的 Ingress 资源）。借助此 API，用户可以轻松管理 Istio 服务网格中的流量。该 API 允许用户将请求路由到特定版本的服务，为弹性测试注入延迟和失败，添加超时和断路器等等，所有这些功能都不必更改应用程序本身的代码。&lt;/p&gt;&#xA;&lt;p&gt;虽然目前 API 的功能已被证明是 Istio 非常引人注目的一部分，但用户的反馈也表明，这个 API 确实有一些缺点，尤其是在使用它来管理包含数千个服务的非常大的应用程序，以及使用 HTTP 以外的协议时。 此外，使用 Kubernetes Ingress 资源来配置外部流量的方式已被证明不能满足需求。&lt;/p&gt;&#xA;&lt;p&gt;为了解决上述缺陷和其他的一些问题，Istio 引入了新的流量管理 API v1alpha3，新版本的 API 将完全取代之前的 API。 尽管 v1alpha3 和之前的模型在本质上是基本相同的，但它并不向后兼容的，基于旧 API 的模型需要进行手动转换。 Istio 接下来的几个版本中会提供一个新旧模型的转换工具。&lt;/p&gt;&#xA;&lt;p&gt;为了证明该非兼容升级的必要性，v1alpha3 API 经历了漫长而艰苦的社区评估过程，以希望新的 API 能够大幅改进，并经得起时间考验。 在本文中，我们将介绍新的配置模型，并试图解释其后面的一些动机和设计原则。&lt;/p&gt;&#xA;&lt;h2 class=&#34;group head-tag&#34; id=&#34;设计原则&#34;&gt;设计原则&lt;a data-title-of-head class=&#34;group-hover:after:content-[&#39;__#&#39;] no-underline hover:text-blue-700&#34; href=&#34;#%e8%ae%be%e8%ae%a1%e5%8e%9f%e5%88%99&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;路由模型的重构过程中遵循了一些关键的设计原则：&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;除支持声明式（意图）配置外，也支持显式指定模型依赖的基础设施。例如，除了配置入口网关（的功能特性）之外，负责实现 入口网关功能的组件（Controller）也可以在模型指定。&lt;/li&gt;&#xA;&lt;li&gt;编写模型时应该“生产者导向”和“以 Host 为中心”，而不是通过组合多个规则来编写模型。 例如，所有与特定 Host 关联的规则被配置在一起，而不是单独配置。&lt;/li&gt;&#xA;&lt;li&gt;将路由与路由后行为清晰分开。&lt;/li&gt;&#xA;&lt;/ul&gt;</description>
    </item>
    <item>
      <title>Istio 0.8 Release发布</title>
      <link>http://localhost:1313/2018/06/02/istio08/</link>
      <pubDate>Sat, 02 Jun 2018 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/2018/06/02/istio08/</guid>
      <description>在 6 月 1 日这一天的早上，Istio 社区宣布发布 0.8 Release，除了常规的故障修复和性能改进外，这个儿童节礼物里面还有什么值得期待内容呢？让我们来看一看：&#xA;Networking改进的流量管理模型0.8 版本采用了新的流量管理配置模型&#xD;v1alpha3 Route API&#xD;。新版本的模型添加了一些新的特性，并改善了之前版本模型中的可用性问题。主要的改动包括：&#xA;Gateway新版本中不再使用 K8s 中的 Ingress，转而采用 Gateway 来统一配置 Service Mesh 中的各个 HTTP/TCP 负载均衡器。Gateway 可以是处理入口流量的 Ingress Gateway，负责 Service Mesh 内部各个服务间通信的 Sidecar Proxy，也可以是负责出口流量的 Egress Gateway。&#xA;Mesh 中涉及到的三类 Gateway:&#xA;该变化的原因是 K8s 中的 Ingress 对象功能过于简单，不能满足 Istio 灵活的路由规则需求。在 0.8 版本中，L4-L6 的配置和 L7 的配置被分别处理，Gateway 中只配置 L4-L6 的功能，例如暴露的端口，TLS 设置。然后用户可以采用 VirtualService 来配置标准的 Istio 规则，并和 Gateway 进行绑定。&#xA;VirtualService采用 VirtualService 代替了 alpha2 模型中的 RouteRule。采用 VirtualService 有两个优势：&#xA;可以把一个服务相关的规则放在一起管理&#xA;例如下面的路由规则，发向 reviews 的请求流量缺省 destination 为 v1，如果 user 为 jason 则路由到 v2。在 v1 模型中需要采用两条规则来实现，采用 VirtualService 后放到一个规则下就可以实现。</description>
    </item>
    <item>
      <title>Istio Sidecar自动注入原理</title>
      <link>http://localhost:1313/2018/05/23/istio-auto-injection-with-webhook/</link>
      <pubDate>Wed, 23 May 2018 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/2018/05/23/istio-auto-injection-with-webhook/</guid>
      <description>&lt;h2 class=&#34;group head-tag&#34; id=&#34;前言&#34;&gt;前言&lt;a data-title-of-head class=&#34;group-hover:after:content-[&#39;__#&#39;] no-underline hover:text-blue-700&#34; href=&#34;#%e5%89%8d%e8%a8%80&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;hr&gt;&#xA;&lt;p&gt;Kubernets 1.9 版本引入了 Admission Webhook(web 回调)扩展机制，通过 Webhook,开发者可以非常灵活地对 Kubernets API Server 的功能进行扩展，在 API Server 创建资源时对资源进行验证或者修改。&lt;/p&gt;&#xA;&lt;p&gt;使用 webhook 的优势是不需要对 API Server 的源码进行修改和重新编译就可以扩展其功能。插入的逻辑实现为一个独立的 web 进程，通过参数方式传入到 kubernets 中，由 kubernets 在进行自身逻辑处理时对扩展逻辑进行回调。&lt;/p&gt;&#xA;&lt;p&gt;Istio 0.7 版本就利用了 Kubernets webhook 实现了 sidecar 的自动注入。&lt;/p&gt;</description>
    </item>
    <item>
      <title>谈谈微服务架构中的基础设施：Service Mesh与Istio</title>
      <link>http://localhost:1313/2018/03/29/what-is-service-mesh-and-istio/</link>
      <pubDate>Thu, 29 Mar 2018 12:00:00 +0000</pubDate>
      <guid>http://localhost:1313/2018/03/29/what-is-service-mesh-and-istio/</guid>
      <description>微服务架构的演进作为一种架构模式，微服务将复杂系统切分为数十乃至上百个小服务，每个服务负责实现一个独立的业务逻辑。这些小服务易于被小型的软件工程师团队所理解和修改，并带来了语言和框架选择灵活性，缩短应用开发上线时间，可根据不同的工作负载和资源要求对服务进行独立缩扩容等优势。&#xA;另一方面，当应用被拆分为多个微服务进程后，进程内的方法调用变成了了进程间的远程调用。引入了对大量服务的连接、管理和监控的复杂性。&#xA;该变化带来了分布式系统的一系列问题，例如：&#xA;如何找到服务的提供方？ 如何保证远程方法调用的可靠性？ 如何保证服务调用的安全性？ 如何降低服务调用的延迟？ 如何进行端到端的调试？ 另外生产部署中的微服务实例也增加了运维的难度,例如：&#xA;如何收集大量微服务的性能指标已进行分析？ 如何在不影响上线业务的情况下对微服务进行升级？ 如何测试一个微服务集群部署的容错和稳定性？ 这些问题涉及到成百上千个服务的通信、管理、部署、版本、安全、故障转移、策略执行、遥测和监控等，要解决这些微服务架构引入的问题并非易事。&#xA;让我们来回顾一下微服务架构的发展过程。在出现服务网格之前，我们最开始在微服务应用程序内理服务之间的通讯逻辑，包括服务发现，熔断，重试，超时，加密，限流等逻辑。&#xA;在一个分布式系统中，这部分逻辑比较复杂，为了为微服务应用提供一个稳定、可靠的基础设施层，避免大家重复造轮子，并减少犯错的可能，一般会通过对这部分负责服务通讯的逻辑进行抽象和归纳，形成一个代码库供各个微服务应用程序使用，如下图所示：&#xA;公共的代码库减少了应用程序的开发和维护工作量，降低了由应用开发人员单独实现微服务通讯逻辑出现错误的机率，但还是存在下述问题：&#xA;微服务通讯逻辑对应用开发人员并不透明，应用开发人员需要理解并正确使用代码 库，不能将其全部精力聚焦于业务逻辑。 需要针对不同的语言/框架开发不同的代码库，反过来会影响微服务应用开发语言 和框架的选择，影响技术选择的灵活性。 随着时间的变化，代码库会存在不同的版本，不同版本代码库的兼容性和大量运行 环境中微服务的升级将成为一个难题。 可以将微服务之间的通讯基础设施层和 TCP/IP 协议栈进行类比。TCP/IP 协议栈为操作系统中的所有应用提供基础通信服务，但 TCP/IP 协议栈和应用程序之间并没有紧密的耦合关系，应用只需要使用 TCP/IP 协议提供的底层通讯功能,并不关心 TCP/IP 协议的实现，如 IP 如何进行路由，TCP 如何创建链接等。&#xA;同样地，微服务应用也不应该需要关注服务发现，Load balancing，Retries，Circuit Breaker 等微服务之间通信的底层细节。如果将为微服务提供通信服务的这部分逻辑从应用程序进程中抽取出来，作为一个单独的进程进行部署，并将其作为服务间的通信代理，可以得到如下图所示的架构：&#xA;因为通讯代理进程伴随应用进程一起部署，因此形象地把这种部署方式称为“sidecar”/边车（即三轮摩托的挎斗）。&#xA;应用间的所有流量都需要经过代理，由于代理以 sidecar 方式和应用部署在同一台主机上，应用和代理之间的通讯可以被认为是可靠的。由代理来负责找到目的服务并负责通讯的可靠性和安全等问题。&#xA;当服务大量部署时，随着服务部署的 sidecar 代理之间的连接形成了一个如下图所示的网格，该网格成为了微服务的通讯基础设施层，承载了微服务之间的所有流量，被称之为 Service Mesh（服务网格）。&#xA;_服务网格是一个基础设施层，用于处理服务间通信。云原生应用有着复杂的服务拓扑，服务网格保证请求可以在这些拓扑中可靠地穿梭。在实际应用当中，服务网格通常是由一系列轻量级的网络代理组成的，它们与应用程序部署在一起，但应用程序不需要知道它们的存在。&#xA;_William Morgan _&#xD;WHAT’S A SERVICE MESH? AND WHY DO I NEED ONE? _&#xA;服务网格中有数量众多的 Sidecar 代理，如果对每个代理分别进行设置，工作量将非常巨大。为了更方便地对服务网格中的代理进行统一集中控制，在服务网格上增加了控制面组件。&#xA;这里我们可以类比 SDN 的概念，控制面就类似于 SDN 网管中的控制器，负责路由策略的指定和路由规则下发；数据面类似于 SDN 网络中交换机，负责数据包的转发。</description>
    </item>
    <item>
      <title>Nginx开源Service Mesh组件Nginmesh安装指南</title>
      <link>http://localhost:1313/2018/01/02/nginmesh-install/</link>
      <pubDate>Tue, 02 Jan 2018 12:00:00 +0000</pubDate>
      <guid>http://localhost:1313/2018/01/02/nginmesh-install/</guid>
      <description>&lt;h2 class=&#34;group head-tag&#34; id=&#34;前言&#34;&gt;前言&lt;a data-title-of-head class=&#34;group-hover:after:content-[&#39;__#&#39;] no-underline hover:text-blue-700&#34; href=&#34;#%e5%89%8d%e8%a8%80&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;Nginmesh 是 NGINX 的 Service Mesh 开源项目，用于 Istio 服务网格平台中的数据面代理。它旨在提供七层负载均衡和服务路由功能，与 Istio 集成作为 sidecar 部署，并将以“标准，可靠和安全的方式”使得服务间通信更容易。Nginmesh 在今年底已经连续发布了 0.2 和 0.3 版本，提供了服务发现，请求转发，路由规则，性能指标收集等功能。&lt;/p&gt;</description>
    </item>
    <item>
      <title>采用Istio实现灰度发布(金丝雀发布)</title>
      <link>http://localhost:1313/2017/11/08/istio-canary-release/</link>
      <pubDate>Wed, 08 Nov 2017 15:00:00 +0000</pubDate>
      <guid>http://localhost:1313/2017/11/08/istio-canary-release/</guid>
      <description>灰度发布（又名金丝雀发布）介绍当应用上线以后，运维面临的一大挑战是如何能够在不影响已上线业务的情况下进行升级。做过产品的同学都清楚，不管在发布前做过多么完备的自动化和人工测试，在发布后都会出现或多或少的故障。根据墨菲定律，可能会出错的版本发布一定会出错。&#xA;&amp;ldquo;ANYTHING THAN CAN GO WRONG WILL GO WRONG&amp;rdquo; &amp;ndash;MURPHY&amp;rsquo;S LAW&#xA;因此我们不能寄希望于在线下测试时发现所有潜在故障。在无法百分百避免版本升级故障的情况下，需要通过一种方式进行可控的版本发布，把故障影响控制在可以接受的范围内，并可以快速回退。&#xA;可以通过&#xD;灰度发布（又名金丝雀发布）&#xD;来实现业务从老版本到新版本的平滑过渡，并避免升级过程中出现的问题对用户造成的影响。&#xA;“金丝雀发布”的来源于矿工们用金丝雀对矿井进行空气测试的做法。以前矿工挖煤的时候，矿工下矿井前会先把金丝雀放进去，或者挖煤的时候一直带着金丝雀。金丝雀对甲烷和一氧化碳浓度比较敏感，会先报警。所以大家都用“金丝雀”来搞最先的测试。&#xA;下图中，左下方的少部分用户就被当作“金丝雀”来用于测试新上线的 1.1 版本。如果新版本出现问题，“金丝雀”们会报警，但不会影响其他用户业务的正常运行。&#xA;灰度发布（金丝雀发布）的流程如下：&#xA;准备和生产环境隔离的“金丝雀”服务器。 将新版本的服务部署到“金丝雀”服务器上。 对“金丝雀”服务器上的服务进行自动化和人工测试。 测试通过后，将“金丝雀”服务器连接到生产环境，将少量生产流量导入到“金丝雀”服务器中。 如果在线测试出现问题，则通过把生产流量从“金丝雀”服务器中重新路由到老版本的服务的方式进行回退，修复问题后重新进行发布。 如果在线测试顺利，则逐渐把生产流量按一定策略逐渐导入到新版本服务器中。 待新版本服务稳定运行后，删除老版本服务。 Istio 实现灰度发布(金丝雀发布)的原理从上面的流程可以看到，如果要实现一套灰度发布的流程，需要应用程序和运维流程对该发布过程进行支持，工作量和难度的挑战是非常大的。虽然面对的问题类似，但每个企业或组织一般采用不同的私有化实现方案来进行灰度发布,为解决该问题导致研发和运维花费了大量的成本。&#xA;Istio 通过高度的抽象和良好的设计采用一致的方式解决了该问题，采用 sidecar 对应用流量进行了转发，通过 Pilot 下发路由规则，可以在不修改应用程序的前提下实现应用的灰度发布。&#xA;备注：采用 kubernetes 的&#xD;滚动升级(rolling update)&#xD;功能也可以实现不中断业务的应用升级,但滚动升级是通过逐渐使用新版本的服务来替换老版本服务的方式对应用进行升级，在滚动升级不能对应用的流量分发进行控制，因此无法采用受控地把生产流量逐渐导流到新版本服务中，也就无法控制服务升级对用户造成的影响。&#xA;采用 Istio 后，可以通过定制路由规则将特定的流量（如指定特征的用户）导入新版本服务中，在生产环境下进行测试，同时通过渐进受控地导入生产流量，可以最小化升级中出现的故障对用户的影响。并且在同时存在新老版本服务时，还可根据应用压力对不同版本的服务进行独立的缩扩容，非常灵活。采用 Istio 进行灰度发布的流程如下图所示：&#xA;操作步骤下面采用 Istion 自带的 BookinfoInfo 示例程序来试验灰度发布的流程。&#xA;测试环境安装首先参考&#xD;手把手教你从零搭建 Istio 及 Bookinfo 示例程序&#xD;安装 Kubernetes 及 Istio 控制面。&#xA;因为本试验并不需要安装全部 3 个版本的 reviews 服务，因此如果已经安装了该应用，先采用下面的命令卸载。&#xA;istio-0.2.10/samples/bookinfo/kube/cleanup.sh 部署 V1 版本的服务首先只部署 V1 版本的 Bookinfo 应用程序。由于示例中的 yaml 文件中包含了 3 个版本的 reviews 服务，我们先将 V2 和 V3 版本的 Deployment 从 yaml 文件 istio-0.</description>
    </item>
    <item>
      <title>使用Istio实现应用流量转移</title>
      <link>http://localhost:1313/2017/11/07/istio-traffic-shifting/</link>
      <pubDate>Tue, 07 Nov 2017 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/2017/11/07/istio-traffic-shifting/</guid>
      <description>&lt;p&gt;关于 Istio 的更多内容请参考&#xD;&#xA;&lt;a&#xD;&#xA;  href=&#34;http://istio.doczh.cn/&#34;&#xD;&#xA;  &#xD;&#xA;    target=&#34;_blank&#34;&#xD;&#xA;  &#xD;&#xA;  &gt;istio 中文文档&lt;/a&#xD;&#xA;&gt;&#xD;&#xA;。&lt;/p&gt;&#xA;&lt;p&gt;原文参见&#xD;&#xA;&lt;a&#xD;&#xA;  href=&#34;https://istio.io/docs/tasks/traffic-management/traffic-shifting.html&#34;&#xD;&#xA;  &#xD;&#xA;    target=&#34;_blank&#34;&#xD;&#xA;  &#xD;&#xA;  &gt;Traffic Shifting&lt;/a&#xD;&#xA;&gt;&#xD;&#xA;。&lt;/p&gt;&#xA;&lt;p&gt;本任务将演示如何将应用流量逐渐从旧版本的服务迁移到新版本。通过 Istio，可以使用一系列不同权重的规则（10%，20%，··· 100%）将流量平缓地从旧版本服务迁移到新版本服务。&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
